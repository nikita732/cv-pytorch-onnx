# Итоговый отчет по «PyTorch → ONNX → INT8»
**Студент:** nikita732  
**Репозиторий:** [cv-pytorch-onnx](https://github.com/nikita732/cv-pytorch-onnx)

---

## 1. Подготовка окружения и данных (DZ #0, #1, #2)
- **Окружение:** Был сформирован файл `requirements.txt` для воспроизводимости среды (torch, torchvision, onnxruntime, seaborn).
- **Данные:** Использован датасет **CIFAR-10**. Для достижения целевой точности применены нормализация (`Normalize`) и изменение размера (`Resize(224)`), соответствующие пресету EfficientNet.
- **Аппаратное обеспечение:** Информация о системе зафиксирована в `docs/gpu_info.md`. В связи с лимитами Colab, финальные этапы оптимизации проводились с упором на **CPU-инференс**.

## 2. Обучение и выбор архитектуры (DZ #3, #4)
- **Модель:** Выбрана архитектура **EfficientNet-B3** как оптимальный баланс между количеством параметров и точностью.
- **Обучение:** Внедрен механизм **Early Stopping** для предотвращения переобучения и сохранения лучшего чекпоинта (`checkpoint.tar`).
- **Результат:** Точность (Accuracy) на валидации составила **96.80%**.

## 3. Профилирование и Латенция (DZ #5, #6)
- **Latency:** Проведен замер задержки на одну картинку (100 прогонов). Результаты визуализированы для оценки стабильности модели.
- **Throughput:** Измерена пропускная способность модели в режиме инференса. Для ONNX на CPU показатель составил **10.24 img/sec**. Был исследован провайдер `CPUExecutionProvider` как основной инструмент для серверного инференса без GPU.

## 4. Конвертация в ONNX и Визуализация (DZ #7)
- **Экспорт:** Модель успешно конвертирована в формат `.onnx` с использованием `opset_version=13/18`.
- **Сравнение:** С помощью библиотеки **Seaborn** построен график `docs/speedup.png`.
- **Вывод:** ONNX Runtime обеспечивает более предсказуемое время отклика (latency) по сравнению с "сырым" PyTorch на CPU, что критично для Real-time систем.

## 5. Квантизация и анализ потерь (DZ #8)
- **Метод:** Применена динамическая квантизация весов в **INT8**.
- **Размер:** Вес модели снизился с **~45 МБ до 11.6 МБ** (сжатие почти в 4 раза).
- **Точность:** Зафиксировано падение точности до **10.60%**.
- **Анализ:** Эксперимент показал, что EfficientNet чувствителен к потере точности весов. Для сохранения метрик необходимо использовать статическую квантизацию с калибровкой (Static Quantization) вместо динамической.

## 6. Итоговое заключение
Проект охватил весь цикл ML-инженерии: от настройки `requirements.txt` до глубокой оптимизации модели. Главный инсайт: оптимизация — это всегда компромисс между скоростью/размером и точностью. Формат ONNX подтвердил свою эффективность как универсальный стандарт для деплоя.

---
*Графики и подробные логи доступны в папке /docs данного репозитория.*
